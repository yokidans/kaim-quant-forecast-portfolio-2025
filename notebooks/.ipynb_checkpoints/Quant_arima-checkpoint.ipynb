{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Quantitative ARIMA Modeling with Ensemble Approaches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "import joblib\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import pickle\n",
    "import json\n",
    "from prophet import Prophet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "# Use importlib.metadata to avoid pkg_resources deprecation\n",
    "try:\n",
    "    from importlib.metadata import version as get_version\n",
    "except ImportError:\n",
    "    from importlib_metadata import version as get_version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Setup Environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ensure directories exist\n",
    "required_dirs = [\"logs\", \"models\", \"reports/figures\", \"data/processed\"]\n",
    "for d in required_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(Path(\"logs/quant_arima.log\")), logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Define QuantitativeARIMAModel Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class QuantitativeARIMAModel:\n",
    "    def __init__(self, ticker=\"TSLA\"):\n",
    "        self.ticker = ticker\n",
    "        self.arima_model = None\n",
    "        self.garch_model = None\n",
    "        self.prophet_model = None\n",
    "        self.lstm_model = None\n",
    "        self.xgb_model = None\n",
    "        self.results = {}\n",
    "        self.feature_importance = None\n",
    "        self.xgb_n_features = None\n",
    "        self.xgb_lookback = None\n",
    "\n",
    "    def load_data(self, fallback_days=365 * 5):\n",
    "        \"\"\"Load data from local or Yahoo Finance.\"\"\"\n",
    "        try:\n",
    "            local_path = Path(\"data/processed/combined_adj_and_returns.csv\")\n",
    "            if local_path.exists():\n",
    "                df = pd.read_csv(local_path, index_col=0, parse_dates=True)\n",
    "                if f\"{self.ticker}_adj\" in df.columns:\n",
    "                    ts = df[[f\"{self.ticker}_adj\"]].copy()\n",
    "                    return self._add_technical_indicators(ts)\n",
    "\n",
    "            logger.warning(\"Local data not found, using Yahoo Finance...\")\n",
    "            data = yf.download(self.ticker, period=f\"{fallback_days}d\", progress=False)\n",
    "            if data.empty:\n",
    "                raise ValueError(\"No data from Yahoo Finance\")\n",
    "\n",
    "            price_col = data[\"Adj Close\"] if \"Adj Close\" in data.columns else data[\"Close\"]\n",
    "            ts = pd.DataFrame(price_col.rename(f\"{self.ticker}_adj\"))\n",
    "            return self._add_technical_indicators(ts)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Data loading failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _add_technical_indicators(self, ts):\n",
    "        \"\"\"Add technical indicators.\"\"\"\n",
    "        if isinstance(ts, pd.Series):\n",
    "            ts = ts.to_frame(name=\"price\")\n",
    "\n",
    "        ts[\"returns\"] = np.log(ts.iloc[:, 0] / ts.iloc[:, 0].shift(1))\n",
    "        ts[\"volatility\"] = ts[\"returns\"].rolling(21).std() * np.sqrt(252)\n",
    "\n",
    "        if \"rsi\" not in ts.columns:\n",
    "            delta = ts.iloc[:, 0].diff()\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            avg_gain = gain.rolling(14).mean()\n",
    "            avg_loss = loss.rolling(14).mean()\n",
    "            ts[\"rsi\"] = 100 - (100 / (1 + (avg_gain / avg_loss)))\n",
    "\n",
    "        if \"macd\" not in ts.columns:\n",
    "            ema12 = ts.iloc[:, 0].ewm(span=12).mean()\n",
    "            ema26 = ts.iloc[:, 0].ewm(span=26).mean()\n",
    "            ts[\"macd\"] = ema12 - ema26\n",
    "\n",
    "        if \"bollinger_middle\" not in ts.columns:\n",
    "            ts[\"bollinger_middle\"] = ts.iloc[:, 0].rolling(20).mean()\n",
    "            ts[\"bollinger_upper\"] = ts[\"bollinger_middle\"] + (ts.iloc[:, 0].rolling(20).std() * 2)\n",
    "            ts[\"bollinger_lower\"] = ts[\"bollinger_middle\"] - (ts.iloc[:, 0].rolling(20).std() * 2)\n",
    "\n",
    "        ts = ts.dropna()\n",
    "        return ts\n",
    "\n",
    "    def prepare_data(self, df, train_end=\"2023-12-31\", test_end=\"2025-07-31\"):\n",
    "        \"\"\"Prepare train/test data.\"\"\"\n",
    "        train_end = pd.to_datetime(train_end)\n",
    "        test_end = pd.to_datetime(test_end)\n",
    "\n",
    "        df[\"returns\"] = np.log(df.iloc[:, 0] / df.iloc[:, 0].shift(1))\n",
    "        df[\"volatility\"] = df[\"returns\"].rolling(21).std() * np.sqrt(252)\n",
    "        df[\"momentum\"] = df.iloc[:, 0] / df.iloc[:, 0].shift(21) - 1\n",
    "\n",
    "        if \"rsi\" not in df.columns:\n",
    "            delta = df.iloc[:, 0].diff()\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            avg_gain = gain.rolling(14).mean()\n",
    "            avg_loss = loss.rolling(14).mean()\n",
    "            df[\"rsi\"] = 100 - (100 / (1 + (avg_gain / avg_loss)))\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        train = df[df.index <= train_end]\n",
    "        test = df[(df.index > train_end) & (df.index <= test_end)]\n",
    "\n",
    "        self._plot_features(df)\n",
    "        self.train_data = train\n",
    "        self.test_data = test\n",
    "        return train, test\n",
    "\n",
    "    def _plot_features(self, df):\n",
    "        \"\"\"Plot indicators.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f\"{self.ticker} Technical Indicators\", y=1.02)\n",
    "\n",
    "        axes[0, 0].plot(df.iloc[:, 0])\n",
    "        axes[0, 0].set_title(\"Price\")\n",
    "\n",
    "        if \"rsi\" in df.columns:\n",
    "            axes[0, 1].plot(df[\"rsi\"])\n",
    "            axes[0, 1].axhline(30, color=\"r\", linestyle=\"--\")\n",
    "            axes[0, 1].axhline(70, color=\"r\", linestyle=\"--\")\n",
    "            axes[0, 1].set_title(\"RSI\")\n",
    "\n",
    "        if \"macd\" in df.columns:\n",
    "            axes[1, 0].plot(df[\"macd\"])\n",
    "            axes[1, 0].set_title(\"MACD\")\n",
    "\n",
    "        bb_cols = [\"bollinger_upper\", \"bollinger_middle\", \"bollinger_lower\"]\n",
    "        if all(col in df.columns for col in bb_cols):\n",
    "            axes[1, 1].plot(df[bb_cols])\n",
    "            axes[1, 1].set_title(\"Bollinger Bands\")\n",
    "\n",
    "        if \"volatility\" in df.columns:\n",
    "            axes[2, 1].plot(df[\"volatility\"])\n",
    "            axes[2, 1].set_title(\"Volatility\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(\"reports/figures/technical_indicators.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    def train_arima_garch(self, train_data):\n",
    "        try:\n",
    "            self.arima_model = auto_arima(\n",
    "                train_data.iloc[:, 0],\n",
    "                seasonal=True,\n",
    "                m=1,\n",
    "                stepwise=True,\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"ignore\",\n",
    "                trace=True,\n",
    "                n_jobs=-1,\n",
    "                information_criterion=\"aic\",\n",
    "                test=\"adf\",\n",
    "                max_order=10,\n",
    "            )\n",
    "\n",
    "            residuals = train_data.iloc[:, 0] - self.arima_model.predict_in_sample()\n",
    "            self.garch_model = arch_model(residuals, vol=\"Garch\", p=1, q=1, dist=\"normal\").fit(disp=\"off\")\n",
    "\n",
    "            logger.info(f\"ARIMA Model Summary:\\n{self.arima_model.summary()}\")\n",
    "            logger.info(f\"GARCH Model Summary:\\n{self.garch_model.summary()}\")\n",
    "\n",
    "            return self.arima_model, self.garch_model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_prophet(self, train_data):\n",
    "        try:\n",
    "            prophet_df = pd.DataFrame({\"ds\": train_data.index, \"y\": train_data.iloc[:, 0]})\n",
    "            available_indicators = [\"rsi\", \"macd\", \"bollinger_upper\", \"bollinger_lower\", \"volatility\"]\n",
    "\n",
    "            self.prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\n",
    "\n",
    "            for indicator in available_indicators:\n",
    "                if indicator in train_data.columns:\n",
    "                    prophet_df[indicator] = train_data[indicator].values\n",
    "                    self.prophet_model.add_regressor(indicator)\n",
    "                    logger.info(f\"Added {indicator} as Prophet regressor\")\n",
    "\n",
    "            prophet_df = prophet_df.dropna()  # NaN guard\n",
    "            self.prophet_model.fit(prophet_df)\n",
    "            return self.prophet_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prophet training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_lstm(self, train_data, lookback=60):\n",
    "        try:\n",
    "            price_data = train_data.iloc[:, 0].values.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(price_data)\n",
    "\n",
    "            X, y = [], []\n",
    "            for i in range(lookback, len(scaled_data)):\n",
    "                X.append(scaled_data[i - lookback : i, 0])\n",
    "                y.append(scaled_data[i, 0])\n",
    "            X, y = np.array(X), np.array(y)\n",
    "            X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "            self.lstm_model = Sequential([\n",
    "                LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "                LSTM(50),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            self.lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "            early_stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "            self.lstm_model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "            logger.info(\"LSTM model trained successfully\")\n",
    "            return self.lstm_model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LSTM training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train_xgboost(self, train_data, lookback=21):\n",
    "        X, y = [], []\n",
    "        n_features = train_data.shape[1]\n",
    "\n",
    "        for i in range(lookback, len(train_data)):\n",
    "            X.append(train_data.iloc[i - lookback : i].values.flatten())\n",
    "            y.append(train_data.iloc[i, 0])\n",
    "\n",
    "        X, y = np.array(X), np.array(y)\n",
    "\n",
    "        self.xgb_model = XGBRegressor(\n",
    "            n_estimators=1000, learning_rate=0.01, max_depth=5, subsample=0.8, colsample_bytree=0.8,\n",
    "            early_stopping_rounds=20, objective=\"reg:squarederror\"\n",
    "        )\n",
    "        self.xgb_model.fit(X, y, eval_set=[(X, y)], verbose=0)\n",
    "\n",
    "        self.xgb_n_features = n_features\n",
    "        self.xgb_lookback = lookback\n",
    "        return self.xgb_model\n",
    "\n",
    "    def ensemble_predict(self, test_data, train_data, lookback=60):\n",
    "        try:\n",
    "            arima_fc = self.arima_model.predict(n_periods=len(test_data))\n",
    "\n",
    "            garch_var = self.garch_model.forecast(horizon=len(test_data)).variance.iloc[-1].values\n",
    "\n",
    "            future = self.prophet_model.make_future_dataframe(\n",
    "                periods=len(test_data),\n",
    "                include_history=False\n",
    "            )\n",
    "\n",
    "            # Add all regressors that Prophet was trained with\n",
    "            for indicator in [\"rsi\", \"macd\", \"bollinger_upper\", \"bollinger_lower\", \"volatility\"]:\n",
    "                if indicator in test_data.columns:\n",
    "                    future[indicator] = test_data[indicator].values\n",
    "                else:\n",
    "                    # If missing, fill with mean or 0 to avoid Prophet errors\n",
    "                    logger.warning(f\"{indicator} missing in test data, filling with mean value\")\n",
    "                    future[indicator] = test_data[indicator].mean() if indicator in test_data else 0\n",
    "\n",
    "            prophet_fc = self.prophet_model.predict(future)[\"yhat\"].values\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            full_data = pd.concat([train_data, test_data])\n",
    "            scaled_data = scaler.fit_transform(full_data.iloc[:, 0].values.reshape(-1, 1))\n",
    "\n",
    "            X_test = []\n",
    "            for i in range(len(train_data), len(full_data)):\n",
    "                X_test.append(scaled_data[i - lookback : i])\n",
    "            X_test = np.array(X_test)\n",
    "            lstm_fc = self.lstm_model.predict(X_test).flatten()\n",
    "            lstm_fc = scaler.inverse_transform(lstm_fc.reshape(-1, 1)).flatten()\n",
    "\n",
    "            X_test_xgb = []\n",
    "            for i in range(len(train_data), len(full_data)):\n",
    "                X_test_xgb.append(full_data.iloc[i - self.xgb_lookback : i].values.flatten())\n",
    "            X_test_xgb = np.array(X_test_xgb)\n",
    "            xgb_fc = self.xgb_model.predict(X_test_xgb)\n",
    "\n",
    "            min_len = min(len(arima_fc), len(prophet_fc), len(lstm_fc), len(xgb_fc))\n",
    "            arima_fc, prophet_fc, lstm_fc, xgb_fc = arima_fc[-min_len:], prophet_fc[-min_len:], lstm_fc[-min_len:], xgb_fc[-min_len:]\n",
    "            test_data = test_data.iloc[-min_len:]\n",
    "\n",
    "            weights = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "            ensemble_fc = (weights[0] * arima_fc + weights[1] * prophet_fc + weights[2] * lstm_fc + weights[3] * xgb_fc)\n",
    "\n",
    "            return {\n",
    "                \"arima\": arima_fc, \"prophet\": prophet_fc, \"lstm\": lstm_fc, \"xgb\": xgb_fc,\n",
    "                \"ensemble\": ensemble_fc, \"garch_var\": garch_var, \"weights\": weights\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ensemble prediction failed: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def save_all_models(self, models_dir: Path):\n",
    "        \"\"\"Save all trained models to disk.\"\"\"\n",
    "        if self.arima_model:\n",
    "            joblib.dump(self.arima_model, models_dir / f\"{self.ticker}_arima.pkl\")\n",
    "        if self.garch_model:\n",
    "            with open(models_dir / f\"{self.ticker}_garch.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.garch_model, f)\n",
    "        if self.prophet_model:\n",
    "            joblib.dump(self.prophet_model, models_dir / f\"{self.ticker}_prophet.pkl\")\n",
    "        if self.lstm_model:\n",
    "            self.lstm_model.save(models_dir / f\"{self.ticker}_lstm.keras\")\n",
    "        if self.xgb_model:\n",
    "            joblib.dump(self.xgb_model, models_dir / f\"{self.ticker}_xgb.pkl\")\n",
    "        logger.info(f\"✅ All models saved to {models_dir}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Main Execution Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def main(ticker, train_end, test_end):\n",
    "    try:\n",
    "        logger.info(f\"Starting quantitative modeling for {ticker}\")\n",
    "        model = QuantitativeARIMAModel(ticker)\n",
    "        df = model.load_data()\n",
    "        train, test = model.prepare_data(df, train_end, test_end)\n",
    "\n",
    "        if not train.columns.equals(test.columns):\n",
    "            raise ValueError(\"Train/test data features do not match\")\n",
    "\n",
    "        model.train_arima_garch(train)\n",
    "        model.train_prophet(train)\n",
    "        model.train_lstm(train)\n",
    "        model.train_xgboost(train)\n",
    "\n",
    "        predictions = model.ensemble_predict(test, train, lookback=21)\n",
    "\n",
    "        actual = test.iloc[:, 0].values[-len(predictions[\"ensemble\"]):]\n",
    "        metrics = {\n",
    "            \"mae\": mean_absolute_error(actual, predictions[\"ensemble\"]),\n",
    "            \"rmse\": np.sqrt(mean_squared_error(actual, predictions[\"ensemble\"])),\n",
    "            \"mape\": mean_absolute_percentage_error(actual, predictions[\"ensemble\"]) * 100,\n",
    "            \"direction_accuracy\": np.mean(\n",
    "                np.sign(np.diff(actual)) == np.sign(np.diff(predictions[\"ensemble\"]))\n",
    "            ) * 100\n",
    "        }\n",
    "\n",
    "        returns = np.log(test.iloc[:, 0] / test.iloc[:, 0].shift(1)).dropna().values\n",
    "        risk = {\n",
    "            \"var_normal\": float(norm.ppf(0.05) * np.sqrt(predictions[\"garch_var\"][-1])),\n",
    "            \"var_t\": float(t.ppf(0.05, df=5) * np.sqrt(predictions[\"garch_var\"][-1])),\n",
    "            \"es\": float(returns[returns < np.percentile(returns, 5)].mean())\n",
    "        }\n",
    "\n",
    "        # Save outputs\n",
    "        models_dir = Path(\"models\")\n",
    "        models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Fix length mismatch for DataFrame\n",
    "        max_len = max(len(v) for v in predictions.values())\n",
    "        for k, v in predictions.items():\n",
    "            # Convert to list and pad with NaN if shorter\n",
    "            v_list = list(v)\n",
    "            if len(v_list) < max_len:\n",
    "                v_list += [np.nan] * (max_len - len(v_list))\n",
    "            predictions[k] = v_list\n",
    "\n",
    "        # Save predictions CSV\n",
    "        pred_df = pd.DataFrame(predictions)\n",
    "        pred_csv_path = models_dir / f\"{ticker}_predictions.csv\"\n",
    "        pred_df.to_csv(pred_csv_path, index=False)\n",
    "\n",
    "        # Save performance & risk metrics JSON\n",
    "        metrics_json_path = models_dir / f\"{ticker}_metrics.json\"\n",
    "        with open(metrics_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"performance\": metrics, \"risk\": risk}, f, indent=4)\n",
    "\n",
    "        # Save all models\n",
    "        model.save_all_models(models_dir)\n",
    "\n",
    "        # Log saved files\n",
    "        logger.info(f\"All models saved to: {models_dir}\")\n",
    "        logger.info(f\"Predictions file: {pred_csv_path}\")\n",
    "        logger.info(f\"Metrics file: {metrics_json_path}\")\n",
    "        return {\"metrics\": metrics, \"risk_metrics\": risk, \"predictions\": predictions}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Run the Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Quantitative ARIMA Modeling Pipeline\")\n",
    "    parser.add_argument(\"--ticker\", default=\"TSLA\", help=\"Stock ticker symbol\")\n",
    "    parser.add_argument(\"--train_end\", default=\"2023-12-31\", help=\"Training end date\")\n",
    "    parser.add_argument(\"--test_end\", default=\"2025-07-31\", help=\"Test end date\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        results = main(args.ticker, args.train_end, args.test_end)\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        print(pd.DataFrame(results[\"metrics\"], index=[0]).T)\n",
    "        print(\"\\n=== Risk Metrics ===\")\n",
    "        print(pd.DataFrame(results[\"risk_metrics\"], index=[0]).T)\n",
    "        print(f\"\\nAll outputs saved to models directory\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script execution failed: {str(e)}\")\n",
    "        sys.exit(1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}